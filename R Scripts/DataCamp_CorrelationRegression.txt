### DATACAMP - CORRELATION & REGRESSION ###


response/dependent variable y
explanatory/independent variable x 

# scatterplot
ggplot(data = possum, aes(y = totalL, x = tailL)) +
	geom_point() +
	scale_x_continuous("Length of Possum Tail (cm)") +
	scale_y_continuous("Length of Possum Body (cm)")

# boxplot
ggplot(data = possum, aes(y = totalL, x = cut(tailL, breaks = 5))) 
	+ geom_boxplot()
	
# discretize continuous variable
cut(variable, breaks = x)

# by colour
ggplot(data = bdims, aes(y = wgt, x = hgt, color=factor(sex) )) + 
	geom_point()
	
# transformation of variable
(1) The coord_trans() function transforms the coordinates of the plot. 
(2) Alternatively, scale_x_log10() and scale_y_log10() perform a base-10 log transformation of each axis

ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point() + 
  coord_trans(x = "log10", y = "log10")

ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10()

# transparency & jitter (= show all points)
ggplot(data = mlbBat10, aes(x = SB, y = HR)) +
    geom_point(alpha = 0.5, position = "jitter")
	
# identify outliers
mlbBat10 %>%
    filter(SB > 60 | HR > 50) %>%
	select(name, team, position, SB, HR)
	
# filter outliers
ab_gt_200 <- mlbBat10 %>%
  filter(AB >= 200) 

ggplot(ab_gt_200, aes(x = OBP, y = SLG)) +
  geom_point()

# Identify the outlying player
ab_gt_200 %>%
  filter(OBP < 0.2)

# correlation (catches only LINEAR relationship!)
between -1 and 1
cor(x,y) #Pearson correlation

ncbirths %>%
  summarize(N = n(), r = cor(weight, mage))

# Compute correlation for all non-missing pairs
summarize(N = n(), r = cor(weight, weeks, use = "pairwise.complete.obs"))

# Anscombe dataset
ggplot(data = Anscombe, aes(x = x, y = y)) + 
	geom_point() + 
	facet_wrap(~ set)

Anscombe %>%
  group_by(set) %>%
  summarize(
    N = n(), 
    mean_of_x = mean(x), 
    std_dev_of_x = sd(x), 
    mean_of_y = mean(y), 
    std_dev_of_y = sd(y), 
    correlation_between_x_and_y = cor(x,y)
  )

mammals %>%
  summarize(N = n(), 
            r = cor(BodyWt, BrainWt), 
            r_log = cor(log(BodyWt), log(BrainWt)))

# plot by segment
ggplot(data = noise, aes(x = x, y = y)) + 
	geom_point() + 
	facet_wrap(~ z)

# Compute correlations for each dataset
noise_summary <- noise %>%
  group_by(z) %>%
  summarize(N = n(), spurious_cor = cor(x, y))

# Isolate sets with correlations above 0.2 in absolute strength
noise_summary %>%
  filter(abs(spurious_cor) > 0.2)

# scatterplot with custom line
ggplot(data = possum, aes(y = totalL, x = cut(tailL, breaks = 5))) 
	+ geom_boxplot() + geom_abline(intercept = 0, slope = 2.5)

# regression line, incl grey standard error area
ggplot(data = possum, aes(y = totalL, x = cut(tailL, breaks = 5))) 
	+ geom_point() + geom_smooth(method = "lm")
	
ggplot(data = possum, aes(y = totalL, x = cut(tailL, breaks = 5))) 
	+ geom_point() + geom_smooth(method = "lm", se = FALSE)  # without standard error area
	
# best fit line = minimum sum of squared errors

# y = actual value
# y-hat = predicted/estimated value

# beta-hats = estimates of true, unknown betas (coefficients)
# residuals = estimates of true, unknown epsilons (error or noise)

# scatterplot + regression line + diagonal
ggplot(data = Galton_men, aes(x = father, y = height)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
  geom_smooth(method = "lm", se = FALSE)
  
# interpretation of linear model
lm(formula = unclaNew ~ amazNew, data = textbooks)
lm(SLG ~ OBP, data = mlbBat10)
lm(log(BodyWt) ~ log(BrainWt), data = mammals) # Log-linear model for body weight as a function of brain weight

# 
class(mod) = "lm"
coef(mod) # coefficients
summary(mod) # summary of model
fitted.values(mod) # fitted values of model
residuals(mod) # residuals/errors

# show fitted values AND original values
library(broom)
augment(mod)

# The least squares fitting procedure guarantees that the mean of the residuals is zero 
# The mean of the fitted values must equal the mean of the response variable.

# make predictions : out-of-sample
predict(lm) # fitted values
predict(lm, newdata) # new predictions

new_data <- data.frame(amazNew = 8.49)
predict(mod, newdata = new_data)

# augmented data of predictions
isrs <- broom::augment(mod, newdata = new_data)
ggplot(data = textbooks, aes(x = amazNew, y = uclaNew)) +
	geom_point() + geom_smooth(method = "lm") + 
	geom_point(data = isrs, aes(y = .fitted), size = 3, color = "red")

# assessing model fit
SSE = sum of squared errors
RMSE = root mean squared error # = standard deviation of residuals # k.a. "residual standard error"
degrees of freedom = n-x

# degrees of freedom
df.residual(mod)

# coefficient of determination : the higher the better quality fit of model
R2 = 1 - SSE/SST = 1 - Var(e)/Var(y) # explained response variability by model
SST = total sum of squares for null model (predicted yhat = average y) # = baseline

"All models are wrong, but some are useful" (George Box)

# Compute R-squared
bdims_tidy %>%
  summarize(var_y = var(bdims_tidy$wgt), var_e = var(residuals(mod))) %>%
  mutate(R_squared = 1 -  var_e / var_y)
  
# Compute SSE for regression model
mod_hgt %>%
  summarize(SSE = var(.resid))

# leverage & influence
leverage = potential ability to influence slope of regression line # = distance from average predictor variable (x)
# observations close to the mean explanatory variable have low leverage, observations far from mean have high leverage
influence = impact of observation on slope  # depends on magnitude of residual, also takes into account response variable
Cook's distance = measure of influence of observation

# # Rank points of high leverage
mod %>%
  augment() %>%
  arrange( desc(.hat)) %>%
  head()

# Rank influential points
mod %>%
  augment() %>%
  arrange( desc(.cooksd)) %>%
  head()

# Create nontrivial_players
nontrivial_players <-
  mlbBat10 %>%
  filter(AB >= 10 & OBP < 0.5)

# Fit model to new data
mod_cleaner <- lm(formula = SLG ~ OBP, data = nontrivial_players)

# View model summary
summary(mod_cleaner)

# Visualize new model
ggplot(data = nontrivial_players, aes(x = OBP, y = SLG)) + 
  geom_point() + geom_smooth(method = "lm")

# Rank high leverage, low influential points
mod %>%
  augment() %>%
  arrange(desc(.hat), .cooksd) %>%
  head()
  
  








