WEB SCRAPING IN PYTHON (SCRAPY) 

What are businesses looking for?
- Comparing prices
- Satisfaction of customers
- Generating potential leads
- ...


## XPATH NOTATION  ##

xpath = "/html/body/div[2]"

simple Xpath :
- single forward-slash '/' used to move forward one generation
- tag-names between slashes give direction to which element(s)
- brackets [] after a tag name tell us which of the selected siblings to choose.

# direct to all 'table' elements within the entire HTML code
xpath = '//table'

# direct to all 'table' elements which are descendants of the 2nd div child of the body element
xpath = '/html/body/div[2]//table'
 
# direct to all 'div' elements with id "uid"
xpath = '//div[@id="uid"]'

# Single forward slash '/' looks forward one generation
# Double forward slash '//' looks forward all future generations
# Square brackets '[]' help narrow in on speciic elements

# Asterisks '*' is wildcard
xpath = /html/body/*'

# @ represents "attribute"

# contains( @attri-name, "string-expr" )  # substring of attribute
xpath = '//*[contains(@class,"class-1"]'   # all elements with "class-1" as part of classes 
xpath = '//*[@class="class-1"]'   # only elements with class == "class-1"

# Go to attribute
xpath = '/html/body/div/p[2]/@class'

xpath = '//p[@id="p2"]/a/@href' # href attribute of id element "p2"

# all href's
xpath = '/html/body//a/@href'
xpath = '//a[contains(@class,"course-block")]/@href'



## SCRAPY SELECTOR ##

# Get HTML into Selector object
from scrapy import Selector
import requests
import requests
url = 'https://www.datacamp.com/courses/all'
html = requests.get( url ).content
sel = Selector(text=html) 

sel.xpath("//p")
# outputs the SelectorList:
[<Selector xpath='//p' data='<p>Hello World!</p>'>, 
 <Selector xpath='//p' data='<p>Enjoy DataCamp!</p>'>]

>>> sel.xpath("//p")
out: [<Selector xpath='//p' data='<p>Hello World!</p>'>,
      <Selector xpath='//p' data='<p>Enjoy DataCamp!</p>'>]

>>> sel.xpath("//p").extract()
out: [ '<p>Hello World!</p>', 
       '<p>Enjoy DataCamp!</p>' ]

>>> sel.xpath("//p").extract_first()   # first element
out: '<p>Hello World!</p>'


# chaining xpath methods
sel.xpath('/html/body/div[2]')
==
sel.xpath('/html').xpath('./body').xpath('./div[2]')



## CSS LOCATORS ##

/ replace by > (except first character)
XPath: /html/body/div
CSS Locator: html > body > div

// replaced by a blank space (except first character)
XPath: //div/span//p
CSS Locator: div > span p

[N] replaced by :nth-of-type(N)
XPath: //div/p[2]
CSS Locator: div > p:nth-of-type(2)

To find an element by class, use a period .
Example: p.class-1 selects all paragraph elements belonging to class-1

To find an element by id, use a pound sign #
Example: div#uid selects the div element with id equal to uid

Select paragraph elements within class class1:
css_locator = 'div#uid > p.class1'

Select all elements whose class attribute belonges to class1:
css_locator = '.class1'


from scrapy import Selector
sel = Selector( text=html)

>>> sel.css("div > p")
out: [<Selector xpath='...' data='<p>Hello World!</p>'>] 

>>> sel.css("div > p").extract()
out: [ '<p>Hello World!</p>' ]


# XPath == CSS
xpath = '//div[@id="uid"]/span//h4'
css_locator = 'div#uid > span h4'

xpath = '/html/body/span[1]//a'
css_locator = 'html > body > span:nth-of-type(1) a'


# Attribute extraction #
Using XPath: <xpath-to-element>/@attr-name
 xpath = '//div[@id="uid"]/a/@href'

Using CSS Locator: <css-to-element>::attr(attr-name)
 css_locator = 'div#uid > a::attr(href)'


# Text extraction #

In XPath use text()
 sel.xpath('//p[@id="p-example"]/text()').extract()
# result: ['\n Hello world!\n Try ', ' today!\n']
 sel.xpath('//p[@id="p-example"]//text()').extract()   # incl text of all children
# result: ['\n Hello world!\n Try ', 'DataCamp', ' today!\n']

For CSS Locator, use ::text
 sel.css('p#p-example::text').extract()
# result: ['\n Hello world!\n Try ', ' today!\n']
 sel.css('p#p-example ::text').extract()   # extra space for including text of all children 
# result: ['\n Hello world!\n Try ', 'DataCamp', ' today!\n']


# Response object

The response keeps track of the URL within the response url variable.
 response.url
>>> 'http://www.DataCamp.com/courses/all'
The response lets us "follow" a new link with the follow() method
 # next_url is the string path of the next url we want to scrape
response.follow( next_url )

xpath method works like a Selector
 response.xpath( '//div/span[@class="bio"]' )
css method works like a Selector
 response.css( 'div > span.bio' )
Chaining works like a Selector
 response.xpath('//div').css('span.bio')
Data extraction works like a Selector
 response.xpath('//div').css('span.bio').extract()
response.xpath('//div').css('span.bio').extract_first()



## SCRAPY SPIDER ##

"""
import scrapy
from scrapy.crawler import CrawlerProcess

class SpiderClassName(scrapy.Spider):
    name = "spider_name"
    # the code for your spider
    ...

# initiate a CrawlerProcess
process = CrawlerProcess()

# tell the process which spider to use
process.crawl(YourSpider)

# start the crawling process
process.start()
"""

class DCspider( scrapy.Spider ):

    name = 'dc_spider'

    def start_requests( self ):
        urls = [ 'https://www.datacamp.com/courses/all' ]
        for url in urls:
            yield scrapy.Request( url = url, callback = self.parse )

    def parse( self, response ):
        # simple example: write out the html
        html_file = 'DC_courses.html'
        with open( html_file, 'wb' ) as fout:
            fout.write( response.body )


- Need to have a function called start_requests
- Need to have at least one parser function to handle the HTML code


# Import scrapy library
import scrapy

# Create the spider class
class YourSpider( scrapy.Spider ):
  name = "your_spider"
  # start_requests method
  def start_requests( self ):
    urls = ["https://www.datacamp.com" , "https://scrapy.org"]
    for url in urls:
      yield url
  # parse method
  def parse( self, response ):
    pass
  
# Inspect Your Class
inspect_class( YourSpider )



# Save to file
class DCspider( scrapy.Spider ):
    name = "dcspider"

    def start_requests( self ):
        urls = [ 'https://www.datacamp.com/courses/all' ]
        for url in urls:
            yield scrapy.Request( url = url, callback = self.parse )

    def parse( self, response ):

        links = response.css('div.course-block > a::attr(href)').extract()
        filepath = 'DC_links.csv'
        with open( filepath, 'w' ) as f:
            f.writelines( [link + '/n' for link in links] )


# Follow links

class DCspider( scrapy.Spider ):
    name = "dcspider"

    def start_requests( self ):
        urls = [ 'https://www.datacamp.com/courses/all' ]
        for url in urls:
            yield scrapy.Request( url = url, callback = self.parse )

    def parse( self, response ):
        links = response.css('div.course-block > a::attr(href)').extract()
        for link in links:
            yield response.follow( url = link, callback = self.parse2 )

    def parse2( self, response ):
        # parse the course sites here!



# Import the scrapy library
import scrapy

# Create the Spider class
class DCdescr( scrapy.Spider ):
  name = 'dcdescr'
  # start_requests method
  def start_requests( self ):
    yield scrapy.Request( url = url_short, callback = self.parse )
  
  # First parse method
  def parse( self, response ):
    links = response.css( 'div.course-block > a::attr(href)' ).extract()
    # Follow each of the extracted links
    for link in links:
      yield response.follow( url = link, callback = self.parse_descr)
      
  # Second parsing method
  def parse_descr( self, response ):
    # Extract course description
    course_descr = response.css( 'p.course__description::text' ).extract_first()
    # For now, just yield the course description
    yield course_descr



## EXAMPLE ##
import scrapy
from scrapy.crawler import CrawlerProcess

# Create the Spider class
class DC_Chapter_Spider(scrapy.Spider):
  name = "dc_chapter_spider"
  # start_requests method
  def start_requests(self):
    yield scrapy.Request(url = url_short,
                         callback = self.parse_front)
  # First parsing method
  def parse_front(self, response):
    course_blocks = response.css('div.course-block')
    course_links = course_blocks.xpath('./a/@href')
    links_to_follow = course_links.extract()
    for url in links_to_follow:
      yield response.follow(url = url,
                            callback = self.parse_pages)
  # Second parsing method
  def parse_pages(self, response):
    crs_title = response.xpath('//h1[contains(@class,"title")]/text()')
    crs_title_ext = crs_title.extract_first().strip()
    ch_titles = response.css('h4.chapter__title::text')
    ch_titles_ext = [t.strip() for t in ch_titles.extract()]
    dc_dict[ crs_title_ext ] = ch_titles_ext

# Initialize the dictionary **outside** of the Spider class
dc_dict = dict()

# Run the Spider
process = CrawlerProcess()
process.crawl(DC_Chapter_Spider)
process.start()

# Print a preview of courses
previewCourses(dc_dict)



## EXAMPLE 2 ##

# Import scrapy
import scrapy

# Import the CrawlerProcess
from scrapy.crawler import CrawlerProcess

# Create the Spider class
class YourSpider(scrapy.Spider):
  name = 'yourspider'
  # start_requests method
  def start_requests( self ):
    yield scrapy.Request(url = url_short, callback = self.parse)
      
  def parse(self, response):
    # My version of the parser you wrote in the previous part
    crs_titles = response.xpath('//h4[contains(@class,"block__title")]/text()').extract()
    crs_descrs = response.xpath('//p[contains(@class,"block__description")]/text()').extract()
    for crs_title, crs_descr in zip( crs_titles, crs_descrs ):
      dc_dict[crs_title] = crs_descr
    
# Initialize the dictionary **outside** of the Spider class
dc_dict = dict()

# Run the Spider
process = CrawlerProcess()
process.crawl(YourSpider)
process.start()

# Print a preview of courses
previewCourses(dc_dict)



## SCRAPING SKILLS ##

Objective: Scrape a website computationally
How? We decide to use scrapy
How? We need to work with:
	Selector and Response objects
	Maybe even create a Spider
How? We need to learn XPath or CSS Locator notation

Structure of HTML
XPath and CSS Locator notation
How to use Selector and Response objects in scrapy
How to set up a spider
How to scrape the web













