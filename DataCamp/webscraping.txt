# flat files from the web
from urllib.request import urlretrieve
url = "http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
urlretrieve(url, 'winequality-white.csv')  # GET HTTP request

# directly in pandas
pd.read_csv(url, sep=',')

# Read in all sheets of Excel file: xl
xl = pd.read_excel(url, sheetname=None)   # for all sheets: sheetname=None
xl.keys() #contains sheetnames

# GET request urllib
from urllib.request import urlopen, Request
url = 'https://www.wikipedia.org/'
request = Request(url)
response = urlopen(request)
html = response.read()
response.close()

# requests library (higher level, no need to close response)
import requests
url = 'url_link'
r = requests.get(url)
text = r.text

# BeautifulSoup 
from bs4 import BeautifulSoup
import requests
url = 'https://www.crummy.com/software/BeautifulSoup/'
r = requests.get(url)
html_doc = r.text
soup = BeautifulSoup(html_doc)

soup.prettify()   # shows HTML tags on seperate lines
soup.find_all()

for link in soup.find_all('a'):   # prints all links in HTMl file
    print(link.get('href'))

soup.title   #get HTML title
soup.get_text()  #get full HTML text

# JSON (JavaScript Object Notation)
import json 
with open('snakes.json', 'r') as json_file:
    json_data = json.load(json_file)
type(json_data)
>> dict

for k,v in json_data.items():
    print(k + ':', v)

# API (Application Programming Interface)
import requests
url = 'http://www.omdbapi.com/?t=hackers'
r = request.get(url)
json_data = r.json()
for k,v in json_data.items():
    print(k + ':', value)


# Twitter API & tweepy
import tweepy, json
access_token = "..."
access_token_secret = "..."
consumer_key = "..."
consumer_secret = "..."

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)


# function tweepy
https://gist.github.com/hugobowne/18f1c0c0709ed1a52dc5bcd462ac69f4
class MyStreamListener(tweepy.StreamListener):
    def __init__(self, api=None):
	super(MyStreamListener, self).__init__()
	self.num_tweets = 0
	self.file = open("tweets.txt", "w")

    def on_status(self, status):
	tweet = status._json
	self.file.write(json.dumps(tweet) + '\n')
	tweet_list.append(status)
	self.num_tweets += 1
	if self.num_tweets < 100:
	    return True
	else:
	    return False
	self.file.close()

#create streaming object and authenticate & filter Twitter Streams to capture data by keywords
l = MyStreamListener()
stream = tweepy.Stream(auth, l)
stream.filter(track=['apples','oranges'])
















